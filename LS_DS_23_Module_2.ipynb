{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR4As_OXvriE"
      },
      "source": [
        "BloomTech Data Science\n",
        "\n",
        "*Unit 2, Sprint 3, Module 2*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Wrangle ML datasets\n",
        "\n",
        "- [ ] Continue to clean and explore your data. \n",
        "- [ ] For the evaluation metric you chose, what score would you get just by guessing?\n",
        "- [ ] Can you make a fast, first model that beats guessing?\n",
        "\n",
        "**We recommend that you use your portfolio project dataset for all assignments this sprint.**\n",
        "\n",
        "**But if you aren't ready yet, or you want more practice, then use the New York City property sales dataset for today's assignment.** Follow the instructions below, to just keep a subset for the Tribeca neighborhood, and remove outliers or dirty data. [Here's a video walkthrough](https://youtu.be/pPWFw8UtBVg?t=584) you can refer to if you get stuck or want hints!\n",
        "\n",
        "- Data Source: [NYC OpenData: NYC Citywide Rolling Calendar Sales](https://data.cityofnewyork.us/dataset/NYC-Citywide-Rolling-Calendar-Sales/usep-8jbt)\n",
        "- Glossary: [NYC Department of Finance: Rolling Sales Data](https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# For Google reproducibility\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = \"/content/drive/My Drive/Kaggle\"\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "    #Connect to remote data\n",
        "    from google.colab import drive\n",
        "    drive.mount(DATA_PATH, force_remount=True)\n",
        "\n",
        "# Local data store on drive D:\n",
        "else:\n",
        "    DATA_PATH = \"D:/Datafiles/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Block\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# #imblearn\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "#sklearn\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.compose import make_column_transformer\n",
        "# from sklearn.compose import make_column_selector as selector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part One - Dataset Basics\n",
        "\n",
        "For this week's modules I am using the asteroid dataset found on Kaggle here: https://www.kaggle.com/datasets/sakhawat18/asteroid-dataset\n",
        "(This is a cleaner version of the NASA/JPL datasets used in my portfolio projects, as such thoroughness is not needed for the daily projects)\n",
        "\n",
        "The target for this exercise will be the Potentially Hazardous Asteroid classification feature. \n",
        "Unfortunately (or, fortunately for the Earth) this feature is heavily unbalanced in favor of 'no' so we will need to adjust our metrics.\n",
        "\n",
        "This is an unbalanced binary classification problem, so we will evaluate balanced accuracy and F1 score, as well as ROC AUC.\n",
        "We will also evaluate the accuracy of the models trained on synthetically balanced datasets using eg. SMOTE techniques.\n",
        "\n",
        "The features in this dataset have a lot of redundancy due to having been merged from various sources; additionally there are\n",
        "many label features which need to be culled for modeling purposes and to prevent cross-leaking. \n",
        "\n",
        "Also worthy of note is that the PHA designation is a direct fuction of two features, absolute magnitude ('H') and minimum orbit intercept \n",
        "distance ('moid'), so models will eventually be compared between the initial inference with the features included, and ones built without \n",
        "those features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrangling Functions\n",
        "\n",
        "def wrangle(filepath):\n",
        "\n",
        "    df = pd.read_csv(filepath, index_col=['pdes']) #pdes = primary designation number\n",
        "    \n",
        "    #drop extraneous label/index/constant columns\n",
        "    labels = ['id', 'spkid', 'full_name', 'name', 'prefix', 'orbit_id', 'equinox', 'class']\n",
        "    df.drop(columns=labels, inplace=True)\n",
        "    #drop duplicate date/distance columns\n",
        "    #NOTE: we want consistent Julian dates & AU distances\n",
        "    labels= ['epoch_mjd', 'epoch_cal', 'tp_cal', 'per_y', 'moid_ld']\n",
        "    df.drop(columns=labels, inplace=True)\n",
        "    #drop leaky columns\n",
        "    labels = ['neo']\n",
        "    df.drop(columns=labels, inplace=True)\n",
        "\n",
        "    #drop rows with no target value\n",
        "    df.dropna(subset=['pha'], inplace=True)\n",
        "\n",
        "    #convert pha to tidy numeric binary encoding\n",
        "    df['pha'] = [1 if flag=='Y' else 0 for flag in df['pha']]\n",
        "\n",
        "    #calculate missing albedo from magnitude\n",
        "    abdmask =  (df['albedo'].isna() & ~(df['H'].isna() | df['diameter'].isna()))\n",
        "    df['albedo'].where(~abdmask, ((1329*(10**((-1)*(df['H']/5)))/df['diameter'])**2), inplace=True) \n",
        "\n",
        "    # impute remaining missing albedo with standard value\n",
        "    # see: https://en.wikipedia.org/wiki/Standard_asteroid_physical_characteristics\n",
        "    df['albedo'].where(~(df['albedo'].isna()), other=0.1, inplace=True)\n",
        "\n",
        "    # calculate missing diameter values from magnitude\n",
        "    diamask = (df['diameter'].isna() & ~(df['H'].isna() | df['albedo'].isna()))\n",
        "    df['diameter'].where(~diamask, (1329/(np.sqrt(df['albedo'])))*(10**((-1)*(df['H']/5))), inplace=True)\n",
        "\n",
        "    #impute missing diameter sigma values\n",
        "    #replacing with maximum error as we are using derived diameter values\n",
        "    df['diameter_sigma'].where(~(df['diameter_sigma'].isna()), other=df['diameter_sigma'].max(), inplace=True)\n",
        "\n",
        "    #finally, calculate missing magnitude values from albedo and diameter\n",
        "    hmask = (df['H'].isna() & ~(df['albedo'].isna() | df['diameter'].isna()))\n",
        "    df['H'].where(~hmask, 5 * (np.log10((1329/(df['diameter'] * np.sqrt(df['albedo']))))), inplace=True)\n",
        "\n",
        "    #any remaining missing values must be dropped\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jeff\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (3,4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(936499, 30)\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 936499 entries, 1 to 2678 T-3\n",
            "Data columns (total 30 columns):\n",
            " #   Column          Non-Null Count   Dtype  \n",
            "---  ------          --------------   -----  \n",
            " 0   pha             936499 non-null  int64  \n",
            " 1   H               936499 non-null  float64\n",
            " 2   diameter        936499 non-null  float64\n",
            " 3   albedo          936499 non-null  float64\n",
            " 4   diameter_sigma  936499 non-null  float64\n",
            " 5   epoch           936499 non-null  float64\n",
            " 6   e               936499 non-null  float64\n",
            " 7   a               936499 non-null  float64\n",
            " 8   q               936499 non-null  float64\n",
            " 9   i               936499 non-null  float64\n",
            " 10  om              936499 non-null  float64\n",
            " 11  w               936499 non-null  float64\n",
            " 12  ma              936499 non-null  float64\n",
            " 13  ad              936499 non-null  float64\n",
            " 14  n               936499 non-null  float64\n",
            " 15  tp              936499 non-null  float64\n",
            " 16  per             936499 non-null  float64\n",
            " 17  moid            936499 non-null  float64\n",
            " 18  sigma_e         936499 non-null  float64\n",
            " 19  sigma_a         936499 non-null  float64\n",
            " 20  sigma_q         936499 non-null  float64\n",
            " 21  sigma_i         936499 non-null  float64\n",
            " 22  sigma_om        936499 non-null  float64\n",
            " 23  sigma_w         936499 non-null  float64\n",
            " 24  sigma_ma        936499 non-null  float64\n",
            " 25  sigma_ad        936499 non-null  float64\n",
            " 26  sigma_n         936499 non-null  float64\n",
            " 27  sigma_tp        936499 non-null  float64\n",
            " 28  sigma_per       936499 non-null  float64\n",
            " 29  rms             936499 non-null  float64\n",
            "dtypes: float64(29), int64(1)\n",
            "memory usage: 221.5+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "df = wrangle(DATA_PATH + \"/Asteroids/dataset.csv\")\n",
        "\n",
        "print(df.shape)\n",
        "print(\"\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part Two - Baseline Modeling\n",
        "\n",
        "Baseline accuracy of ~0.998 is a bit problematic, so we will try \n",
        "1. Different scoring methods\n",
        "2. Some basic resampling \n",
        "before building the initial models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9977939111520674\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "# Create two datasets to model, with/without the H (magnitude) + moid columns\n",
        "# easy mode = default columns, hard mode = without, model must infer from other data\n",
        "df_hard = df.drop(columns=['H', 'moid'])\n",
        "\n",
        "X = df.drop(columns='pha')\n",
        "y = df['pha']\n",
        "\n",
        "X_h = df_hard.drop(columns='pha')\n",
        "y_h = df_hard['pha']\n",
        "\n",
        "#naive baseline scores\n",
        "model_dum = DummyClassifier(strategy='prior').fit(X, y)\n",
        "dum_pred = model_dum.predict(X)\n",
        "baseline_acc = accuracy_score(y, dum_pred)\n",
        "print(baseline_acc)\n",
        "base_bal_acc = balanced_accuracy_score(y, dum_pred)\n",
        "print(base_bal_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(749199, 29)\n",
            "(749199, 29)\n",
            "(749199,)\n",
            "(749199,)\n",
            "(749199, 27)\n",
            "(749199, 27)\n",
            "(749199,)\n",
            "(749199,)\n"
          ]
        }
      ],
      "source": [
        "#split datasets into train, test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_h_train, X_h_test, y_h_train, y_h_test = train_test_split(X_h, y_h, test_size=0.2, random_state=42)\n",
        "\n",
        "#get majority/minority class splits for resampling\n",
        "class_mask = (df['pha'] == 1)\n",
        "X_train_min, X_train_maj = X_train.loc[class_mask], X_train.loc[~class_mask]\n",
        "X_h_train_min, X_h_train_maj = X_h_train.loc[class_mask], X_h_train.loc[~class_mask]\n",
        "\n",
        "#evenly over- and under-sample dataset to create balanced training set\n",
        "pha_y, pha_n = y_train.value_counts(sort=True, ascending=True)\n",
        "\n",
        "train_midp_sample_1 = X_train_min.sample(((pha_n + pha_y)//2), replace=True)\n",
        "y_sample_1 = pd.Series([1] * len(train_midp_sample_1))\n",
        "train_midp_sample_0 = X_train_maj.sample(((pha_n + pha_y)//2)+1, replace=False)\n",
        "y_sample_0 = pd.Series([0] * len(train_midp_sample_0))\n",
        "X_train_res =  train_midp_sample_1.append(train_midp_sample_0, ignore_index=True)\n",
        "y_train_res = pd.concat([y_sample_1, y_sample_0], ignore_index=True)\n",
        "\n",
        "pha_h_y, pha_h_n = y_h_train.value_counts(sort=True, ascending=True)\n",
        "\n",
        "train_h_midp_sample_1 = X_h_train_min.sample(((pha_n + pha_y)//2), replace=True)\n",
        "y_h_sample_1 = pd.Series([1] * len(train_h_midp_sample_1))\n",
        "train_h_midp_sample_0 = X_h_train_maj.sample(((pha_h_n + pha_h_y)//2)+1, replace=False)\n",
        "y_h_sample_0 = pd.Series([0] * len(train_h_midp_sample_0))\n",
        "X_h_train_res =  train_h_midp_sample_1.append(train_h_midp_sample_0, ignore_index=True)\n",
        "y_h_train_res = pd.concat([y_h_sample_1, y_h_sample_0], ignore_index=True)\n",
        "\n",
        "#check for deformed arrays\n",
        "print(X_train.shape)\n",
        "print(X_train_res.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train_res.shape)\n",
        "\n",
        "print(X_h_train.shape)\n",
        "print(X_h_train_res.shape)\n",
        "print(y_h_train.shape)\n",
        "print(y_h_train_res.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######### Original Dataset Distributions ########\n",
            "*Full Data Columns*\n",
            "LR Model Accuracy:  0.9982594767752269\n",
            "LR Model Balanced Accuracy:  0.6768255810981356\n",
            "RF Model Accuracy:  0.999893219434063\n",
            "RF Model Balanced Accuracy:  0.9828633334716426\n",
            "*Limited Data Columns*\n",
            "LR Model Accuracy:  0.9974319273892152\n",
            "LR Model Balanced Accuracy:  0.524940230108523\n",
            "RF Model Accuracy:  0.9983342231713828\n",
            "RF Model Balanced Accuracy:  0.6985016999579061\n",
            "######### Resampled Dataset Distributions ########\n",
            "*Full Data Columns*\n",
            "LR Model Accuracy:  0.9938013881473572\n",
            "LR Model Balanced Accuracy:  0.9957545526356983\n",
            "RF Model Accuracy:  0.9998985584623599\n",
            "RF Model Balanced Accuracy:  0.9965325301229551\n",
            "*Limited Data Columns*\n",
            "LR Model Accuracy:  0.9791510945008008\n",
            "LR Model Balanced Accuracy:  0.9895511125857585\n",
            "RF Model Accuracy:  0.9978056593699947\n",
            "RF Model Balanced Accuracy:  0.8736238165453968\n"
          ]
        }
      ],
      "source": [
        "# Initial models\n",
        "\n",
        "model_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "model_rf = make_pipeline(RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42))\n",
        "\n",
        "print(\"######### Original Dataset Distributions ########\")\n",
        "\n",
        "print(\"*Full Data Columns*\")\n",
        "\n",
        "model_lr.fit(X_train, y_train)\n",
        "lr_acc = accuracy_score(y_test, model_lr.predict(X_test))\n",
        "print(\"LR Model Accuracy: \", lr_acc)\n",
        "lr_acc_bal = balanced_accuracy_score(y_test, model_lr.predict(X_test))\n",
        "print(\"LR Model Balanced Accuracy: \", lr_acc_bal)\n",
        "\n",
        "model_rf.fit(X_train, y_train)\n",
        "rf_acc = accuracy_score(y_test, model_rf.predict(X_test))\n",
        "print(\"RF Model Accuracy: \", rf_acc)\n",
        "rf_acc_bal = balanced_accuracy_score(y_test, model_rf.predict(X_test))\n",
        "print(\"RF Model Balanced Accuracy: \", rf_acc_bal)\n",
        "\n",
        "print(\"*Limited Data Columns*\")\n",
        "\n",
        "model_lr.fit(X_h_train, y_h_train)\n",
        "lr_h_acc = accuracy_score(y_h_test, model_lr.predict(X_h_test))\n",
        "print(\"LR Model Accuracy: \", lr_h_acc)\n",
        "lr_h_acc_bal = balanced_accuracy_score(y_h_test, model_lr.predict(X_h_test))\n",
        "print(\"LR Model Balanced Accuracy: \", lr_h_acc_bal)\n",
        "\n",
        "model_rf.fit(X_h_train, y_h_train)\n",
        "rf_h_acc = accuracy_score(y_h_test, model_rf.predict(X_h_test))\n",
        "print(\"RF Model Accuracy: \", rf_h_acc)\n",
        "rf_h_acc_bal = balanced_accuracy_score(y_h_test, model_rf.predict(X_h_test))\n",
        "print(\"RF Model Balanced Accuracy: \", rf_h_acc_bal)\n",
        "\n",
        "print(\"######### Resampled Dataset Distributions ########\")\n",
        "\n",
        "print(\"*Full Data Columns*\")\n",
        "\n",
        "model_lr.fit(X_train_res, y_train_res)\n",
        "lr_acc = accuracy_score(y_test, model_lr.predict(X_test))\n",
        "print(\"LR Model Accuracy: \", lr_acc)\n",
        "lr_acc_bal = balanced_accuracy_score(y_test, model_lr.predict(X_test))\n",
        "print(\"LR Model Balanced Accuracy: \", lr_acc_bal)\n",
        "\n",
        "model_rf.fit(X_train_res, y_train_res)\n",
        "rf_acc = accuracy_score(y_test, model_rf.predict(X_test))\n",
        "print(\"RF Model Accuracy: \", rf_acc)\n",
        "rf_acc_bal = balanced_accuracy_score(y_test, model_rf.predict(X_test))\n",
        "print(\"RF Model Balanced Accuracy: \", rf_acc_bal)\n",
        "\n",
        "print(\"*Limited Data Columns*\")\n",
        "\n",
        "model_lr.fit(X_h_train_res, y_h_train_res)\n",
        "lr_h_acc = accuracy_score(y_h_test, model_lr.predict(X_h_test))\n",
        "print(\"LR Model Accuracy: \", lr_h_acc)\n",
        "lr_h_acc_bal = balanced_accuracy_score(y_h_test, model_lr.predict(X_h_test))\n",
        "print(\"LR Model Balanced Accuracy: \", lr_h_acc_bal)\n",
        "\n",
        "model_rf.fit(X_h_train_res, y_h_train_res)\n",
        "rf_h_acc = accuracy_score(y_h_test, model_rf.predict(X_h_test))\n",
        "print(\"RF Model Accuracy: \", rf_h_acc)\n",
        "rf_h_acc_bal = balanced_accuracy_score(y_h_test, model_rf.predict(X_h_test))\n",
        "print(\"RF Model Balanced Accuracy: \", rf_h_acc_bal)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "LS_DS_231_assignment_JD.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "687c238b3ce093b6d05593c14198ba6d4a27c356ca33aff629c1e2e7517a29f8"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
